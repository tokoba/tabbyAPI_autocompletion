# TabbyAPI プロジェクト概要

## 1. プロジェクトの目的

TabbyAPIは、大規模言語モデル（LLM）のフロントエンドAPIを提供するために設計された、高性能かつ軽量なバックエンドサーバーです。FastAPIをベースに構築されており、高速なテキスト生成を実現するために、主に`exllamav2`およびその後継である`exllamav3`推論エンジンを採用しています。

本プロジェクトの主な目的は、これらの推論エンジンの公式APIバックエンドとして機能し、OpenAIやKoboldAIといった主要なエコシステムとの互換性を提供しつつ、豊富な設定・管理機能を提供することです。

## 2. 主要機能

TabbyAPIは、LLMのサービス提供と管理のための包括的な機能セットを提供します。

- **マルチAPI互換性**:
  - **OpenAI (OAI)**: `/v1/chat/completions`や`/v1/completions`といったエンドポイントを提供し、OpenAI APIを模倣することで、幅広いクライアントとの互換性を確保します。
  - **KoboldAI (KAI)**: KoboldAI API標準と互換性のあるエンドポイントを提供します。
- **動的なリソース管理**:
  - **モデルのロード/アンロード**: サーバーを再起動することなく、API呼び出しによって動的にモデルをロード・アンロードできます。
  - **マルチLoRAサポート**: 複数のLoRAアダプタを、それぞれ独立したスケーリング係数で同時にロードできます。
- **効率的な推論**:
  - **バックエンドサポート**: `exllamav2`および`exllamav3`という強力な推論バックエンドを活用します。
  - **投機的デコーディング**: 小さなドラフトモデルを使用して、より大きなプライマリモデルでの生成を高速化する機能をサポートします。
  - **並行推論**: `asyncio`を利用して、複数のリクエストを同時に処理します。
  - **継続的バッチ処理**: ページドアテンション（Paged Attention）を利用して、バッチリクエストを効率的に処理します（対応ハードウェアのみ）。
- **高度な生成制御**:
  - **文法制約**: JSONスキーマ、正規表現、EBNFをサポートし、テキスト生成をガイドします。
  - **柔軟なサンプリング**: 多様なサンプリングパラメータ（temperature, top-k, top-pなど）を提供し、プリセットシステムによってデフォルト値を管理・上書きできます。
  - **ツールおよび関数呼び出し**: OpenAIスタイルのツール呼び出しを実装し、モデルが外部ツールと対話できるようにします。
- **柔軟なチャットフォーマット**:
  - **Jinja2テンプレートエンジン**: Jinja2テンプレートを使用してチャット履歴をフォーマットし、HuggingFace標準に準拠させつつ、カスタムプロンプト構造を可能にします。
- **ユーティリティと管理**:
  - **HuggingFaceダウンローダー**: HuggingFace HubからモデルやLoRAを直接ダウンロードするための組み込みユーティリティ。
  - **埋め込みモデル**: `infinity-emb`ライブラリを介して、テキスト埋め込みを生成するための別のモデルのロードをサポートします。
  - **Visionサポート**: Vision（画像認識）をサポートするマルチモーダルモデルを扱う能力があります。

## 3. 技術仕様

### 3.1. アーキテクチャ

アプリケーションはモジュール化されたアーキテクチャで構築されています。

- **Webフレームワーク**: **FastAPI** を使用し、非同期APIを構築。
- **推論バックエンド**: 推論エンジンのためのプラグイン可能なシステム。
  - **プライマリ**: `exllamav2`, `exllamav3`。
  - **埋め込み**: `infinity-emb`。
- **設定**: `config.yml`、環境変数、コマンドライン引数から階層的にロードされる設定システム。
- **APIレイヤー**: エンドポイントは互換性の種類によって整理されています。
  - **Core API**: サーバー管理用（モデル/LoRAのロード、ヘルスチェックなど）。
  - **OAI API**: OpenAI互換用。
  - **KoboldAI API**: KoboldAI互換用。

### 3.2. サポートされるモデル形式

サポートされるモデル形式は、`exllamav2`/`v3`バックエンドによって決まります。

- **EXL2 / EXL3**: `exllamav2`/`v3`に強く推奨される量子化形式。
- **GPTQ**: もう一つの一般的な量子化形式。
- **FP16**: 非量子化の16ビット浮動小数点モデル。

### 3.3. 主要な依存関係

- `fastapi`: Webサーバーフレームワーク。
- `pydantic`: データ検証と設定管理。
- `uvicorn`: ASGIサーバー。
- `torch`: コアとなる機械学習フレームワーク。
- `exllamav2` / `exllamav3`: 推論エンジンのためのPythonホイール。
- `jinja2`: チャットテンプレート用。
- `ruamel.yaml`: YAML設定ファイルの解析用。
- `huggingface_hub`: モデルのダウンロード用。
- `infinity-emb` (オプション): 埋め込みモデル用。

## 4. APIエンドポイントの概要

APIは主に3つのグループに構成されています。

1. **Core API (`/`)**: サーバーを直接管理するためのエンドポイント。
    - `/v1/model/load`, `/v1/model/unload`: メインの推論モデルを管理。
    - `/v1/lora/load`, `/v1/lora/unload`: LoRAアダプタを管理。
    - `/v1/models`, `/v1/loras`: 利用可能なモデルとLoRAを一覧表示。
    - `/health`: サービスのヘルスチェック。
    - `/v1/download`: HuggingFaceからモデルをダウンロード。
2. **OpenAI API (`/v1`)**: OpenAI互換のエンドポイント。
    - `/v1/chat/completions`: チャットベースの生成用。
    - `/v1/completions`: テキスト補完用。
    - `/v1/embeddings`: テキスト埋め込みの生成用。
3. **KoboldAI API (`/api`)**: KoboldAI互換のエンドポイント。
    - `/api/v1/generate`: テキスト生成用。
    - `/api/extra/generate/stream`: ストリーミング形式のテキスト生成用。
